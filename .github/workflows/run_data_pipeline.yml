name: Run Data Pipeline

on:
  workflow_dispatch:
    inputs:
      pipeline_type:
        description: 'Pipeline type to run'
        required: true
        default: 'update'
        type: choice
        options:
          - initialize
          - update
      countries:
        description: 'Comma-separated country codes (e.g., ATG,BLZ,DOM). Leave empty for default.'
        required: false
        type: string
      storm:
        description: 'Specific storm name to process (e.g., JERRY)'
        required: false
        type: string
      forecast_date:
        description: 'Forecast date (YYYYMMDD or YYYY-MM-DD)'
        required: false
        type: string
      forecast_run:
        description: 'Forecast run time (HHMM or HH:MM)'
        required: false
        type: string

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours max (initialize can take 30-60 min per country)
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            gdal-bin \
            libgdal-dev \
            python3-gdal

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          # Create temporary requirements file without giga-spatial
          grep -v "giga-spatial" requirements.txt > /tmp/requirements_temp.txt || true
          pip install -r /tmp/requirements_temp.txt
          # Install giga-spatial separately (may need special handling)
          pip install giga-spatial>=0.7.0

      - name: Create output directories
        run: |
          mkdir -p results geodb/aos_views/{mercator_views,school_views,hc_views,track_views}

      - name: Configure environment variables
        env:
          # Pipeline configuration
          DATA_PIPELINE_DB: ${{ secrets.DATA_PIPELINE_DB || 'BLOB' }}
          RESULTS_DIR: ${{ secrets.RESULTS_DIR || 'results' }}
          BBOX_FILE: ${{ secrets.BBOX_FILE || 'bbox.parquet' }}
          STORMS_FILE: ${{ secrets.STORMS_FILE || 'storms.json' }}
          ROOT_DATA_DIR: ${{ secrets.ROOT_DATA_DIR || 'geodb' }}
          VIEWS_DIR: ${{ secrets.VIEWS_DIR || 'aos_views' }}
          
          # Snowflake credentials
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          
          # Azure Blob Storage (if using BLOB mode)
          ACCOUNT_URL: ${{ secrets.ACCOUNT_URL }}
          SAS_TOKEN: ${{ secrets.SAS_TOKEN }}
          
          # API tokens
          GIGA_SCHOOL_LOCATION_API_KEY: ${{ secrets.GIGA_SCHOOL_LOCATION_API_KEY }}
          HEALTHSITES_API_KEY: ${{ secrets.HEALTHSITES_API_KEY }}
          GEOREPO_API_KEY: ${{ secrets.GEOREPO_API_KEY || '' }}
          GEOREPO_USER_EMAIL: ${{ secrets.GEOREPO_USER_EMAIL || '' }}
        run: |
          echo "Environment configured"

      - name: Initialize bounding box (if needed)
        env:
          # Re-declare env vars for this step
          DATA_PIPELINE_DB: ${{ secrets.DATA_PIPELINE_DB || 'BLOB' }}
          RESULTS_DIR: ${{ secrets.RESULTS_DIR || 'results' }}
          BBOX_FILE: ${{ secrets.BBOX_FILE || 'bbox.parquet' }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          ACCOUNT_URL: ${{ secrets.ACCOUNT_URL }}
          SAS_TOKEN: ${{ secrets.SAS_TOKEN }}
          GIGA_SCHOOL_LOCATION_API_KEY: ${{ secrets.GIGA_SCHOOL_LOCATION_API_KEY }}
          HEALTHSITES_API_KEY: ${{ secrets.HEALTHSITES_API_KEY }}
          GEOREPO_API_KEY: ${{ secrets.GEOREPO_API_KEY || '' }}
          GEOREPO_USER_EMAIL: ${{ secrets.GEOREPO_USER_EMAIL || '' }}
        run: |
          # Check if bounding box exists, create if not
          python -c "
          from data_store_utils import get_data_store
          from config import config
          import os
          data_store = get_data_store()
          bbox_path = os.path.join(config.RESULTS_DIR, config.BBOX_FILE)
          if not data_store.file_exists(bbox_path):
              print('Creating bounding box...')
              from impact_analysis import save_bounding_box
              countries = ['ATG','BLZ','NIC','DOM','DMA','GRD','MSR','KNA','LCA','VCT','AIA','VGB']
              save_bounding_box(countries)
              print('Bounding box created')
          else:
              print('Bounding box already exists')
          "
        continue-on-error: true

      - name: Run pipeline
        env:
          # Re-declare all env vars for this step
          DATA_PIPELINE_DB: ${{ secrets.DATA_PIPELINE_DB || 'BLOB' }}
          RESULTS_DIR: ${{ secrets.RESULTS_DIR || 'results' }}
          BBOX_FILE: ${{ secrets.BBOX_FILE || 'bbox.parquet' }}
          STORMS_FILE: ${{ secrets.STORMS_FILE || 'storms.json' }}
          ROOT_DATA_DIR: ${{ secrets.ROOT_DATA_DIR || 'geodb' }}
          VIEWS_DIR: ${{ secrets.VIEWS_DIR || 'aos_views' }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          ACCOUNT_URL: ${{ secrets.ACCOUNT_URL }}
          SAS_TOKEN: ${{ secrets.SAS_TOKEN }}
          GIGA_SCHOOL_LOCATION_API_KEY: ${{ secrets.GIGA_SCHOOL_LOCATION_API_KEY }}
          HEALTHSITES_API_KEY: ${{ secrets.HEALTHSITES_API_KEY }}
          GEOREPO_API_KEY: ${{ secrets.GEOREPO_API_KEY || '' }}
          GEOREPO_USER_EMAIL: ${{ secrets.GEOREPO_USER_EMAIL || '' }}
        run: |
          # Determine countries to process
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ inputs.countries }}" ]; then
            COUNTRIES="${{ inputs.countries }}"
          else
            COUNTRIES="ATG JAM BLZ NIC DOM DMA GRD MSR KNA LCA VCT AIA VGB"
          fi
          
          # Determine pipeline type
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TYPE="${{ inputs.pipeline_type }}"
          else
            TYPE="update"
          fi

          # If storm, forecast_date and forecast_run provided, run only that target directly
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ inputs.storm }}" ] && [ -n "${{ inputs.forecast_date }}" ] && [ -n "${{ inputs.forecast_run }}" ]; then
            echo "Running targeted pipeline for storm=${{ inputs.storm }}, forecast_date=${{ inputs.forecast_date }}, forecast_run=${{ inputs.forecast_run }}"
            python - << 'PYCODE'
import os
from main_pipeline import run_hurricane_pipeline
import re

storm = os.environ.get('INPUT_STORM') or "${{ inputs.storm }}"
raw_date = os.environ.get('INPUT_FORECAST_DATE') or "${{ inputs.forecast_date }}"
raw_run = os.environ.get('INPUT_FORECAST_RUN') or "${{ inputs.forecast_run }}"
countries = (os.environ.get('INPUT_COUNTRIES') or "${{ inputs.countries }}" or "ATG JAM BLZ NIC DOM DMA GRD MSR KNA LCA VCT AIA VGB").split()

# Normalize date/run to YYYYMMDD and HHMM
date_digits = re.sub(r'\D', '', raw_date or '')
run_digits = re.sub(r'\D', '', raw_run or '')
if len(date_digits) != 8 or len(run_digits) not in (2,4):
    raise SystemExit(f"Invalid forecast_date or forecast_run: date='{raw_date}', run='{raw_run}'")
if len(run_digits) == 2:
    run_digits = f"{run_digits}00"
forecast_time = f"{date_digits}{run_digits}00"

stats = run_hurricane_pipeline(
    storm=storm,
    forecast_time=forecast_time,
    countries=countries,
    skip_analysis=False,
    log_level="INFO",
    zoom=14,
)
print("Done. Success=", stats.analysis_success)
PYCODE
          else
            echo "Running standard pipeline: type=$TYPE"
            python main_pipeline.py \
              --type "$TYPE" \
              --hazard hurricane \
              --zoom 14 \
              --countries $COUNTRIES \
              --log-level INFO
          fi

      - name: Upload results to Azure Blob (if using LOCAL mode)
        env:
          DATA_PIPELINE_DB: ${{ secrets.DATA_PIPELINE_DB || 'BLOB' }}
          ACCOUNT_URL: ${{ secrets.ACCOUNT_URL }}
          SAS_TOKEN: ${{ secrets.SAS_TOKEN }}
        run: |
          # Only upload if using LOCAL mode
          if [ "$DATA_PIPELINE_DB" = "LOCAL" ]; then
            echo "Uploading results to Azure Blob Storage..."
            # Install Azure CLI
            curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
            
            # Upload results directory
            az storage blob upload-batch \
              --account-url "$ACCOUNT_URL" \
              --sas-token "$SAS_TOKEN" \
              --destination results \
              --source results \
              --overwrite || echo "Failed to upload results directory"
            
            # Upload geodb directory
            az storage blob upload-batch \
              --account-url "$ACCOUNT_URL" \
              --sas-token "$SAS_TOKEN" \
              --destination geodb \
              --source geodb \
              --overwrite || echo "Failed to upload geodb directory"
          else
            echo "Skipping upload - using $DATA_PIPELINE_DB mode (results already in Azure)"
          fi
        continue-on-error: true

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs
          path: |
            main_pipeline.log
            results/
          retention-days: 7

